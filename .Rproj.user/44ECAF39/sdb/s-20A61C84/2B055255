{
    "collab_server" : "",
    "contents" : "---\nlayout: post\ntitle: LDAVis Show Case on R-Bloggers\ncomments: true\npublished: true\nauthor: Marcin Kosiński\ncategories: [Text Mining]\noutput:\n  html_document:\n    mathjax:  default\n    fig_caption:  true\n    toc: true\n    section_numbering: true\n    keep_md: true\n---\n```{r include = FALSE}\nlibrary(knitr)\nopts_chunk$set(\n\tcomment = \"\",\n\tfig.width = 12, \n\tmessage = FALSE,\n\twarning = FALSE,\n\ttidy.opts = list(\n\t\tkeep.blank.line = TRUE,\n\t\twidth.cutoff = 150\n\t\t),\n\toptions(width = 150),\n\teval = FALSE\n)\n```\n<img src=\"/images/fulls/LDAvis.png\" class=\"fit image\"> Text mining is a new challenge for machine wandering practitioners. The increased interest in the text mining is caused by an augmentation of internet users and by rapid growth of the internet data which is said that *in 80% is a text data*. Extracting information from articles, news, posts and comments have became a desirable skill but what is even more needful are tools for text mining models diagnostics and visualizations. Such visualizations enable to better understand the insight from a model and provides an easy interface for presenting your research results to greater audience. In this post I present the Latent Dirichlet Allocation text mining model for text classification into topics and a great [LDAvis](https://cran.r-project.org/web/packages/LDAvis/index.html) package for interactive visualizations of topic models. All this on [R-Bloggers](http://r-bloggers.com/) posts!\n\nLDAvis is not my package. It is created by [cpsievert](https://github.com/cpsievert) and this post's code for LDAvis-preparations is mostly based on his LDAvis tutorial: [A topic model for movie reviews](http://cpsievert.github.io/LDAvis/reviews/reviews.html)\n\n# LDA overview and text preparation\n\nFrom [Wikipedia](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n\n> In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics.\n\nFor this post I have used articles from [R-Bloggers](http://r-bloggers.com/). \nThey can be downloaded from [this repository](https://github.com/MarcinKosinski/r-bloggers-harvesting). The data harvesting process is explained at the end of this post. \n\n```{r}\nlibrary(RSQLite)\ndb.conn <- \n  dbConnect(\n    dbDriver(\"SQLite\"),\n    \"r-bloggers.db\"\n)\nposts <- dbGetQuery(db.conn, \n           \"SELECT text from posts\")\ndbDisconnect(db.conn)\n```\n\nNormally I would use `LDA()` function from [topicmodels](https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf) package to fit LDA model because the input can be of class `DocumentTermMatrix` which is an object from [tm](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) package. `DocumentTermMatrix` object is very convinient for working with text data ([check this Norbert Ryciak's post](http://www.rexamine.com/2014/06/text-mining-in-r-automatic-categorization-of-wikipedia-articles/)) because there exists `tm_map` function which can be applied to all documents for stop words removal, lowering capital letters and removal of words that did not occur in x % of documents. I haven't seen `LDAvis` examples for models generated with topicsmodel package so we will use traditional approach to text processing. The [stemming](https://en.wikipedia.org/wiki/Lemmatisation) and stopwords removal was perfored during the data collection, which is described at the end of the post.\n\n```{r}\n## the following fragment of code in this section\n## is motivated by\n## http://cpsievert.github.io/LDAvis/reviews/reviews.html\n# tokenize on space and output as a list:\ndoc.list <- strsplit(posts[, 1], \"[[:space:]]+\")\n# compute the table of terms:\nterm.table <- table(unlist(doc.list))\nterm.table <- sort(term.table, decreasing = TRUE)\n# remove terms that occur fewer than 5 times:\nterm.table <- term.table[term.table > 5]\nvocab <- names(term.table)\n```\n\nThe `lda.collapsed.gibbs.sampler()` function from `tm` package has uncomfortable input format (regarding to `LDA()` from topicmodels package) so I basically used [cpsievert](https://github.com/cpsievert) snippets\n\n```{r}\n\n# now put the documents into the format required by the lda package:\nget.terms <- function(x) {\n  index <- match(x, vocab)\n  index <- index[!is.na(index)]\n  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))\n}\ndocuments <- lapply(doc.list, get.terms)\n\n# Compute some statistics related to the data set:\nD <- length(documents)  # number of documents (3740)\nW <- length(vocab)  # number of terms in the vocab (18,536)\ndoc.length <- sapply(documents, \n                     function(x) sum(x[2, ]))  \n# number of tokens per document [312, 288, 170, 436, 291, ...]\nN <- sum(doc.length)  # total number of tokens in the data (546,827)\nterm.frequency <- as.integer(term.table)  \n# frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]\n```\n\nFitting the model: from `tm` package documentation\n\n> ... [this function] takes sparsely represented input documents, perform inference, and return point estimates of the latent parameters using the state at the last iteration of Gibbs sampling. \n\n```{r}\n# MCMC and model tuning parameters:\nK <- 20\nG <- 5000\nalpha <- 0.02\neta <- 0.02\n\n# Fit the model:\nlibrary(lda)\nset.seed(456)\nt1 <- Sys.time()\nfit <- lda.collapsed.gibbs.sampler(\n  documents = documents, K = K, \n  vocab = vocab, num.iterations = G, \n  alpha = alpha, eta = eta, \n  initial = NULL, burnin = 0,\n  compute.log.likelihood = TRUE\n)\nt2 <- Sys.time()\nt2 - t1  # about 7 seconds on my laptop\n\nlibrary(archivist)\nsaveToRepo(fit,\n           repoDir = \"../Museum\")\n```\n\nThe computations took very long, so in case you would like to \nget model faster, I have archived my model on GitHub with the help of\n[archivist](http://r-bloggers.com/r-hero-saves-backup-city-with-archivist-and-github/) package.\nYou can easily load this model to R with\n```{r}\narchivist::aread('MarcinKosinski/Museum/fa93abf0ff93a7f6f3f0c42b7935ad4d') -> fit\n```\n\n\n# LDAVis use case\n\nIf you google out properly you'll wind out that LDAvis description is\n\n> Tools to create an interactive web-based visualization of a topic model that has been fit to a corpus of text data using Latent Dirichlet Allocation (LDA). Given the estimated parameters of the topic model, it computes various summary statistics as input to an interactive visualization built with D3.js that is accessed via a browser. The goal is to help users interpret the topics in their LDA topic model.\n\n[Detailed vignette about LDAvis input and output can be found here](https://cran.r-project.org/web/packages/LDAvis/vignettes/details.pdf).\n\n> To visualize the result using LDAvis, we'll need estimates of the document-topic distributions, which we denote by the DxK matrix theta, and the set of topic-term distributions, which we denote by the K×W matrix phi. \n\n```{r}\ntheta <- t(apply(fit$document_sums + alpha,\n                 2,\n                 function(x) x/sum(x)))\nphi <- t(apply(t(fit$topics) + eta,\n               2,\n               function(x) x/sum(x)))\n```\n\n```{r}\nlibrary(LDAvis)\n# create the JSON object to feed the visualization:\njson <- createJSON(\n  phi = phi, \n  theta = theta, \n  doc.length = doc.length, \n  vocab = vocab, \n  term.frequency = term.frequency\n)\nserVis(json, out.dir = 'vis', \n       open.browser = FALSE)\n```\n\nThe result is published under this link [http://r-addict.com/r-bloggers-harvesting/](http://r-addict.com/r-bloggers-harvesting/) where you can check Intertopic Distance Map (via multidimensional scaling) and top N relevant terms for a topic.\n\n\n# Data Harvesting\n\nBelow is the code I have used for R-Bloggers web-scraping. At start I have extracted all links to posts\nfrom first 100 main pages of R-Bloggers. Then I have created an SQLite databases with empty table called `posts`.\nThis table has been used to store information like: post title, post link, post author, date of publication and the whole post text. For the text I have extracted only words (with [stringi](http://www.gagolewski.com/software/stringi/) package) that have length greater than 1 and applied `tolower` function to get rid of capital letters. Stop words removal was done thanks to `tm::removeWords()`. For [stemming](https://en.wikipedia.org/wiki/Stemming) I have used [RWeka::LovinsStemmer](https://cran.r-project.org/web/packages/RWeka/RWeka.pdf). I did not perform full lemmatization as I have found it troubling in R (couldn't install [this](http://stackoverflow.com/questions/28214148/how-to-perform-lemmatization-in-r) and [this](http://stackoverflow.com/questions/22993796/lemmatizer-in-r-or-python-am-are-is-be) took too long).\n\n\n```{r}\nlibrary(rvest)\nlibrary(pbapply)\npbsapply(1:100, function(i){\n  read_html(paste0('http://www.r-bloggers.com/page/', i)) %>%\n  html_nodes('h2 a') %>%\n  html_attr('href') %>%\n  grep('r-bloggers', x = ., value = TRUE)\n}) %>% \n  as.vector() %>%\n  unique -> links\n\n\n# create connection\nlibrary(RSQLite)\ndb.conn <- \n  dbConnect(\n    dbDriver(\"SQLite\"),\n    \"r-bloggers.db\"\n)\n# create empty table for posts\nposts <- \n  data.frame(link = \"\",\n             title = \"\",\n             # shares = \"\",\n             text = \"\",\n             date = \"\",\n             author = \"\")\ndbWriteTable(\n  db.conn,\n  name = \"posts\",\n  posts,\n  overwrite = TRUE,\n  row.names = FALSE\n) \n\n# function for info extraction\nlibrary(stringi)\nlibrary(tm)\nlibrary(RWeka)\npostInfo <- function(link, conn) {\n  post <- read_html(link)\n  title <- html_nodes(post, 'h1') %>%\n    html_text() %>%\n    .[1] %>%\n    gsub(\"\\\"\", \"\", x = .) \n  # shares <- html_nodes(post,\n  #   '.wpusb-counts , .wpusb-counts span') %>%\n  #   html_text()\n  text <- html_nodes(post, 'p') %>% \n    html_text() %>% head(-1) %>%\n    paste(collapse=\" \") %>%\n    gsub(\"\\\"\", \"\", x = .) %>%\n    stri_extract_all_words()  %>% \n    .[[1]] \n  text <- text[stri_length(text) > 1] %>%\n    tolower() %>%\n    gsub(\"\\\\.\", \" \", x = .) %>%\n    removeWords(c(stopwords('english'),\n                  stopwords('SMART'),\n                  \"\", \"’s\", \"amp\", \n                  \"div\", \"’ve\", 1:100, \n                  \"’ll\", \"let’s\", \"’re\", \n                  \"’m\")) \n  text <- text[stri_length(text) > 1] %>%\n    stri_extract_all_words() %>% \n    unlist %>%\n    LovinsStemmer(control = NULL) %>%\n    paste(collapse = \" \")\n  \n  date <- html_nodes(post, '.date') %>%\n    html_text() %>%\n    as.Date(format=\"%b %d, %Y\")\n  if (is.na(date)) {\n    date <- \"\"\n  }\n  author <- html_nodes(post, '.meta') %>% \n    html_text() %>% strsplit(\"By \") %>% \n    # second element of length-1 list\n    .[[1]] %>% .[2] %>%\n    gsub(\"\\\"\", \"\", x = .)\n \n  dbGetQuery(\nconn,\npaste0(\"INSERT INTO posts\n(link,title, text, date, author) VALUES\",\n\"(\\\"\", link, \"\\\",\\\"\", title, \"\\\",\\\"\", #shares, \"\\\",\\\"\",\ntext, \"\\\",\\\"\", date, \"\\\",\\\"\", author, \"\\\")\")\n  )   \n}\n\n# for proper date extraction\nlct <- Sys.getlocale(\"LC_TIME\")\nSys.setlocale(\"LC_TIME\", \"C\")\n# get info for all links\npbsapply(\n  links,\n  postInfo, \n  db.conn\n)\nSys.setlocale(\"LC_TIME\", lct)\n# disconnect with database\ndbDisconnect(db.conn)\n       \n\n```\n\nIf you have any questions or comments, please feel free to share your ideas on the Disqus panel below.\nAlso, if you know how to web-scrap the number of shares per R-Bloggers article then I would love to hear your feedback as I am wondering what's the correlation between `Hadley Wickham` appearance in the post and its number of shares.\n\n",
    "created" : 1466422142824.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3630994005",
    "id" : "2B055255",
    "lastKnownWriteTime" : 1466461229,
    "last_content_update" : 1466461229795,
    "path" : "~/MarcinKosinski.github.io/_source/2016-06-21-LDAvis-RBloggers.Rmd",
    "project_path" : null,
    "properties" : {
        "ignored_words" : "SQLite,topicsmodel,topicmodels,LDAvis,Disqus,GitHub,google,lemmatization\n",
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}